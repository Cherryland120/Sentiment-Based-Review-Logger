{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50b5f49c-b017-4fcf-81ec-153957b080b7",
   "metadata": {},
   "source": [
    "# Sentiment-Based Review Logger\n",
    "\n",
    "This project is an AI-powered sentiment analysis system designed to classify user-submitted reviews and log negative feedback for further analysis. Built using **PyTorch**, **TorchText**, and **NLP techniques**, the system prompts users for their name and product review, then processes the input using a **pre-trained sentiment classification model**.\n",
    "\n",
    "## Key Features:\n",
    "- Interactive User Input – Prompts users for their name and review in a Jupyter Notebook environment.\n",
    "- Sentiment Classification – Utilizes a trained deep learning model to categorize reviews as Negative, Positive, or Neutral.\n",
    "- Automated Review Logging – If a review is classified as negative, it is automatically saved in a structured CSV file (negative_reviews.csv).\n",
    "- Scalable Data Handling – Uses pandas for efficient data storage, allowing easy analysis of recurring negative feedback trends.\n",
    "- Customizable File Path – The CSV file can be saved in any specified directory for centralized logging.\n",
    "\n",
    "## Use Cases:\n",
    "- Businesses can track **customer dissatisfaction** trends to improve their products.\n",
    "- AI researchers can **fine-tune NLP models** using real-world feedback.\n",
    "- Developers can **extend the project** to include real-time review monitoring in web applications.\n",
    "\n",
    "## Dependencies\n",
    "- torch\n",
    "- torchtext\n",
    "- spacy ('en_core_web_sm' language model)\n",
    "- pandas\n",
    "\n",
    "Author: Tamunowunari-Tasker Anointing"
   ]
  },
  {
   "cell_type": "code",
   "id": "2bdeb817-5570-48cd-b4c3-1d4553f9842c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T22:40:02.570430Z",
     "start_time": "2026-01-06T22:39:58.148132Z"
    }
   },
   "source": [
    "# Importing necessary libraries from PyTorch and TorchText\n",
    "import torchtext.data.utils as tdu  # For tokenization utilities\n",
    "import torchtext.vocab as tv        # For vocabulary building\n",
    "import torch.nn as nn               # For neural network components like Embedding\n",
    "import torch                        # For tensor operations\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "ebd92f85-8242-4c80-b625-f9caecfc7899",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T22:40:04.977101Z",
     "start_time": "2026-01-06T22:40:02.584285Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('Dataset.csv')  # Replace with your actual filename\n",
    "\n",
    "# Convert DataFrame to list of tuples format (text, label)\n",
    "dataset = list(zip(df['Statement'], df['Sentiment']))\n",
    "\n",
    "print(f\"✓ Loaded {len(dataset)} samples from CSV\")\n",
    "print(f\"  - Positive: {sum(1 for _, label in dataset if label == 1)}\")\n",
    "print(f\"  - Negative: {sum(1 for _, label in dataset if label == 0)}\")\n",
    "print(f\"  - Neutral: {sum(1 for _, label in dataset if label == 2)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 324 samples from CSV\n",
      "  - Positive: 109\n",
      "  - Negative: 110\n",
      "  - Neutral: 105\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "c52c6943-4444-4fa4-9d49-ddce6effaa9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T22:40:09.024197Z",
     "start_time": "2026-01-06T22:40:04.998872Z"
    }
   },
   "source": [
    "# Initialize the tokenizer using SpaCy's small English model\n",
    "tokenizer = tdu.get_tokenizer('spacy', language = 'en_core_web_sm') # get_tokenizer() is unique to torchtext"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "799feb94-cab3-4a55-8b99-c6a8fcf9d629",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T22:40:09.040040Z",
     "start_time": "2026-01-06T22:40:09.033820Z"
    }
   },
   "source": [
    "# Function to yield tokens from each sentence in the dataset\n",
    "def yield_tokens(data_iter):\n",
    "    \"\"\"\n",
    "    Tokenizes each sentence in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        data_iter (iter): An iterator over the dataset.\n",
    "        \n",
    "    Yields:\n",
    "        list: A list of tokens for each sentence.\n",
    "    \"\"\"\n",
    "    for sentence, _  in data_iter:\n",
    "        yield tokenizer(sentence) # gives you one value at a time but remembers where it left off, ready to continue when called again.\n",
    "\n",
    "# Create an iterator over the dataset\n",
    "data_iter = iter(dataset) # iter is an inbuilt function in Python that returns an iterator from a list, tuple or a custom dataset class"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "96a7ce61-c232-4595-a92f-0667a21eeb31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T22:40:09.222595Z",
     "start_time": "2026-01-06T22:40:09.047832Z"
    }
   },
   "source": [
    "# Build vocabulary from the tokenized dataset\n",
    "vocab = tv.build_vocab_from_iterator(\n",
    "    yield_tokens(dataset),\n",
    "    specials=[\"<unk>\"]\n",
    ")\n",
    "\n",
    "# Set <unk> as the default token for unknown words\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "37a7a600-2b9a-432f-9dca-b64cb9fe29bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T22:40:09.263298Z",
     "start_time": "2026-01-06T22:40:09.253288Z"
    }
   },
   "source": [
    "# Display the vocabulary as a list of words (index-to-string mapping)\n",
    "print(vocab.get_itos()) # get_itos() returns the list/bag of words. This results in a bag of 9"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', 'This', 'is', 'this', 'I', 'I‚Äôm', 'work', 'Very', 'done', 'Not', 'okay', 'bad', 'It', 'job', 'It‚Äôs', 'does', 'experience', 'quality', 'result', 'the', 'works', 'Could', 'good', '-', 'disappointing', 'well', 'Absolutely', 'Acceptable', 'Average', 'Awful', 'Brilliant', 'Clean', 'Completely', 'Excellent', 'Extremely', 'Fair', 'Great', 'Highly', 'Just', 'Low', 'Major', 'Mediocre', 'Neither', 'Neutral', 'Nicely', 'No', 'Nothing', 'Outstanding', 'Poorly', 'Really', 'Sloppy', 'So', 'Standard', 'Superb', 'Top', 'Unacceptable', 'Well', 'about', 'amazing', 'and', 'annoying', 'badly', 'broken', 'built', 'decent', 'dislike', 'either', 'elegant', 'enough', 'exceeded', 'execution', 'expectations', 'failed', 'fantastic', 'feelings', 'fine', 'frustrating', 'go', 'implementation', 'impressed', 'impressive', 'indifferent', 'letdown', 'love', 'makes', 'my', 'no', 'nor', 'not', 'notch', 'outcome', 'passable', 'perfectly', 'recommended', 'regret', 'remarkable', 'satisfied', 'sense', 'so', 'special', 'strong', 'terrible', 'time', 'trying', 'unhappy', 'useless', 'very', 'way', 'with', 'wonderful', 'worth', '!', 'like', \"'s\", 'Good', 'Impressive', 'been', 'better', 'guess', 'have', 'it', \"n't\", 'website', 'AI', 'Awesome', 'Heresy', 'Horrible', 'Its', 'Okay', 'Utterly', 'all', 'an', 'do', 'hate', 'hope', 'inspiration', 'looks', 'made', 'make', 'over', 'really', 'something', 'to', 'written']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "893442fd-228a-4e79-a227-f167395d308a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T22:40:09.281192Z",
     "start_time": "2026-01-06T22:40:09.276383Z"
    }
   },
   "source": [
    "# Convert sentences to token indices\n",
    "input_indexes = lambda data: [torch.tensor(vocab(tokenizer(sentence))) for sentence, _ in data]"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "d68b787a-0ba8-4d06-b0de-f3fed703ecd7",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2026-01-06T22:40:09.375764Z",
     "start_time": "2026-01-06T22:40:09.299907Z"
    }
   },
   "source": [
    "# Convert dataset sentences to token indices\n",
    "index = input_indexes(dataset) # index has three tensors\n",
    "print(\"Token indices:\", index)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token indices: [tensor([  1,   2,  22, 111]), tensor([124, 111]), tensor([ 9, 22]), tensor([  4, 133,   3, 111]), tensor([114,   6]), tensor([ 1,  2, 11]), tensor([ 21, 119, 116, 117]), tensor([ 21, 121, 119, 116, 117]), tensor([ 12, 113,  10]), tensor([127,  10]), tensor([128,   4, 118]), tensor([129,  24]), tensor([114,  13]), tensor([126]), tensor([  1, 122,   2,  25, 137]), tensor([  1, 136, 112, 123, 143, 130,  23, 139, 120]), tensor([  4, 132, 121, 112,   3, 122]), tensor([  4, 118, 120, 113,  10]), tensor([  4, 140, 134, 142, 138, 141, 112,   3]), tensor([  1,   2, 131, 135]), tensor([115]), tensor([  9, 115]), tensor([ 9, 11]), tensor([125]), tensor([36,  6]), tensor([  1,   2, 101]), tensor([14, 75]), tensor([33, 13]), tensor([ 7, 24]), tensor([46, 99]), tensor([ 7, 80]), tensor([ 4, 65,  3]), tensor([28, 16]), tensor([ 4, 83,  3]), tensor([48,  8]), tensor([12, 20]), tensor([ 1,  2, 73]), tensor([ 1, 15, 88,  6]), tensor([ 45, 100,  74]), tensor([56,  8]), tensor([  4,  94, 103,   3]), tensor([ 1,  2, 10]), tensor([26, 58]), tensor([29, 16]), tensor([27, 18]), tensor([30, 70]), tensor([ 1, 72, 61]), tensor([41]), tensor([37, 93]), tensor([34, 76]), tensor([42, 22, 87, 11]), tensor([ 1, 69, 85, 71]), tensor([39, 17]), tensor([12, 15, 19, 13]), tensor([49, 25, 63]), tensor([  9, 110,  19, 102]), tensor([51, 23, 98]), tensor([53, 17]), tensor([ 1,  2, 62]), tensor([35, 68]), tensor([ 1, 20, 92]), tensor([55]), tensor([ 21,  77,  66, 107]), tensor([  5, 106,  96]), tensor([ 1, 84, 86, 97]), tensor([ 1,  2, 64]), tensor([54, 89]), tensor([ 7, 11, 78]), tensor([43, 57,  3]), tensor([47, 18]), tensor([50,  6]), tensor([ 5, 81]), tensor([31, 59, 67]), tensor([  5, 104, 108,   3]), tensor([14, 91]), tensor([44,  8]), tensor([ 32, 105]), tensor([ 9, 95]), tensor([  1,   2, 109]), tensor([ 1,  2, 60]), tensor([52, 90]), tensor([ 5, 79]), tensor([40, 82]), tensor([38, 10]), tensor([36,  6]), tensor([  1,   2, 101]), tensor([14, 75]), tensor([33, 13]), tensor([ 7, 24]), tensor([46, 99]), tensor([ 7, 80]), tensor([ 4, 65,  3]), tensor([28, 16]), tensor([ 4, 83,  3]), tensor([48,  8]), tensor([12, 20]), tensor([ 1,  2, 73]), tensor([ 1, 15, 88,  6]), tensor([ 45, 100,  74]), tensor([56,  8]), tensor([  4,  94, 103,   3]), tensor([ 1,  2, 10]), tensor([26, 58]), tensor([29, 16]), tensor([27, 18]), tensor([30, 70]), tensor([ 1, 72, 61]), tensor([41]), tensor([37, 93]), tensor([34, 76]), tensor([42, 22, 87, 11]), tensor([ 1, 69, 85, 71]), tensor([39, 17]), tensor([12, 15, 19, 13]), tensor([49, 25, 63]), tensor([  9, 110,  19, 102]), tensor([51, 23, 98]), tensor([53, 17]), tensor([ 1,  2, 62]), tensor([35, 68]), tensor([ 1, 20, 92]), tensor([55]), tensor([ 21,  77,  66, 107]), tensor([  5, 106,  96]), tensor([ 1, 84, 86, 97]), tensor([ 1,  2, 64]), tensor([54, 89]), tensor([ 7, 11, 78]), tensor([43, 57,  3]), tensor([47, 18]), tensor([50,  6]), tensor([ 5, 81]), tensor([31, 59, 67]), tensor([  5, 104, 108,   3]), tensor([14, 91]), tensor([44,  8]), tensor([ 32, 105]), tensor([ 9, 95]), tensor([  1,   2, 109]), tensor([ 1,  2, 60]), tensor([52, 90]), tensor([ 5, 79]), tensor([40, 82]), tensor([38, 10]), tensor([36,  6]), tensor([  1,   2, 101]), tensor([14, 75]), tensor([33, 13]), tensor([ 7, 24]), tensor([46, 99]), tensor([ 7, 80]), tensor([ 4, 65,  3]), tensor([28, 16]), tensor([ 4, 83,  3]), tensor([48,  8]), tensor([12, 20]), tensor([ 1,  2, 73]), tensor([ 1, 15, 88,  6]), tensor([ 45, 100,  74]), tensor([56,  8]), tensor([  4,  94, 103,   3]), tensor([ 1,  2, 10]), tensor([26, 58]), tensor([29, 16]), tensor([27, 18]), tensor([30, 70]), tensor([ 1, 72, 61]), tensor([41]), tensor([37, 93]), tensor([34, 76]), tensor([42, 22, 87, 11]), tensor([ 1, 69, 85, 71]), tensor([39, 17]), tensor([12, 15, 19, 13]), tensor([49, 25, 63]), tensor([  9, 110,  19, 102]), tensor([51, 23, 98]), tensor([53, 17]), tensor([ 1,  2, 62]), tensor([35, 68]), tensor([ 1, 20, 92]), tensor([55]), tensor([ 21,  77,  66, 107]), tensor([  5, 106,  96]), tensor([ 1, 84, 86, 97]), tensor([ 1,  2, 64]), tensor([54, 89]), tensor([ 7, 11, 78]), tensor([43, 57,  3]), tensor([47, 18]), tensor([50,  6]), tensor([ 5, 81]), tensor([31, 59, 67]), tensor([  5, 104, 108,   3]), tensor([14, 91]), tensor([44,  8]), tensor([ 32, 105]), tensor([ 9, 95]), tensor([  1,   2, 109]), tensor([ 1,  2, 60]), tensor([52, 90]), tensor([ 5, 79]), tensor([40, 82]), tensor([38, 10]), tensor([36,  6]), tensor([  1,   2, 101]), tensor([14, 75]), tensor([33, 13]), tensor([ 7, 24]), tensor([46, 99]), tensor([ 7, 80]), tensor([ 4, 65,  3]), tensor([28, 16]), tensor([ 4, 83,  3]), tensor([48,  8]), tensor([12, 20]), tensor([ 1,  2, 73]), tensor([ 1, 15, 88,  6]), tensor([ 45, 100,  74]), tensor([56,  8]), tensor([  4,  94, 103,   3]), tensor([ 1,  2, 10]), tensor([26, 58]), tensor([29, 16]), tensor([27, 18]), tensor([30, 70]), tensor([ 1, 72, 61]), tensor([41]), tensor([37, 93]), tensor([34, 76]), tensor([42, 22, 87, 11]), tensor([ 1, 69, 85, 71]), tensor([39, 17]), tensor([12, 15, 19, 13]), tensor([49, 25, 63]), tensor([  9, 110,  19, 102]), tensor([51, 23, 98]), tensor([53, 17]), tensor([ 1,  2, 62]), tensor([35, 68]), tensor([ 1, 20, 92]), tensor([55]), tensor([ 21,  77,  66, 107]), tensor([  5, 106,  96]), tensor([ 1, 84, 86, 97]), tensor([ 1,  2, 64]), tensor([54, 89]), tensor([ 7, 11, 78]), tensor([43, 57,  3]), tensor([47, 18]), tensor([50,  6]), tensor([ 5, 81]), tensor([31, 59, 67]), tensor([  5, 104, 108,   3]), tensor([14, 91]), tensor([44,  8]), tensor([ 32, 105]), tensor([ 9, 95]), tensor([  1,   2, 109]), tensor([ 1,  2, 60]), tensor([52, 90]), tensor([ 5, 79]), tensor([40, 82]), tensor([38, 10]), tensor([36,  6]), tensor([  1,   2, 101]), tensor([14, 75]), tensor([33, 13]), tensor([ 7, 24]), tensor([46, 99]), tensor([ 7, 80]), tensor([ 4, 65,  3]), tensor([28, 16]), tensor([ 4, 83,  3]), tensor([48,  8]), tensor([12, 20]), tensor([ 1,  2, 73]), tensor([ 1, 15, 88,  6]), tensor([ 45, 100,  74]), tensor([56,  8]), tensor([  4,  94, 103,   3]), tensor([ 1,  2, 10]), tensor([26, 58]), tensor([29, 16]), tensor([27, 18]), tensor([30, 70]), tensor([ 1, 72, 61]), tensor([41]), tensor([37, 93]), tensor([34, 76]), tensor([42, 22, 87, 11]), tensor([ 1, 69, 85, 71]), tensor([39, 17]), tensor([12, 15, 19, 13]), tensor([49, 25, 63]), tensor([  9, 110,  19, 102]), tensor([51, 23, 98]), tensor([53, 17]), tensor([ 1,  2, 62]), tensor([35, 68]), tensor([ 1, 20, 92]), tensor([55]), tensor([ 21,  77,  66, 107]), tensor([  5, 106,  96]), tensor([ 1, 84, 86, 97]), tensor([ 1,  2, 64]), tensor([54, 89]), tensor([ 7, 11, 78]), tensor([43, 57,  3]), tensor([47, 18]), tensor([50,  6]), tensor([ 5, 81]), tensor([31, 59, 67]), tensor([  5, 104, 108,   3]), tensor([14, 91]), tensor([44,  8]), tensor([ 32, 105]), tensor([ 9, 95]), tensor([  1,   2, 109]), tensor([ 1,  2, 60]), tensor([52, 90]), tensor([ 5, 79]), tensor([40, 82]), tensor([38, 10])]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "435fc9fc-5f76-42f4-aa2d-dc93465dce59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T22:40:09.400550Z",
     "start_time": "2026-01-06T22:40:09.389384Z"
    }
   },
   "source": [
    "# Extract sentiment labels\n",
    "labels = torch.tensor([label for _, label in dataset])"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "8b51cb7d-e31b-406e-b04c-6888e2a37176",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T22:40:09.422150Z",
     "start_time": "2026-01-06T22:40:09.416420Z"
    }
   },
   "source": [
    "# Set embedding dimensions (number of features each word vector will have)\n",
    "embedding_dim = 3 # Each word will be represented in a 3D vector space\n",
    "\n",
    "# Number of unique words in the vocabulary\n",
    "n_embedding = len(vocab)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "d46cecac-9279-44d0-be88-c47f7a5b01c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T22:40:09.448371Z",
     "start_time": "2026-01-06T22:40:09.436176Z"
    }
   },
   "source": [
    "# Initialize EmbeddingBag layer to get sentence-level embeddings\n",
    "embedding_bag = nn.EmbeddingBag(num_embeddings=n_embedding, embedding_dim=embedding_dim, mode='mean')"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "85d2358f-6c19-4146-88e1-695cb6c4f53d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T22:40:09.483522Z",
     "start_time": "2026-01-06T22:40:09.466413Z"
    }
   },
   "source": [
    "# Prepare flattened token indices and offsets for EmbeddingBag\n",
    "index_flat = torch.cat(index)\n",
    "offsets = torch.tensor([0] + [len(sample) for sample in index[:-1]]).cumsum(0)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "33e2da2a-efa7-44c5-bffb-08d89af09b7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T22:40:09.502263Z",
     "start_time": "2026-01-06T22:40:09.495652Z"
    }
   },
   "source": [
    "# Define a simple sentiment classifier model\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, embedding_bag, embedding_dim, num_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.embedding_bag = embedding_bag\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)  # Linear layer for classification\n",
    "        \n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding_bag(text, offsets)  # Get sentence embeddings\n",
    "        return self.fc(embedded)  # Pass through linear layer"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "804dcd73-77b1-4048-9451-7beba3668c61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T22:40:09.518065Z",
     "start_time": "2026-01-06T22:40:09.511991Z"
    }
   },
   "source": [
    "# Instantiate the model\n",
    "num_classes = 3  # Positive, Negative, Neutral\n",
    "model = SentimentClassifier(embedding_bag, embedding_dim, num_classes)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "28f25b8a-6b75-4891-b05e-0b49c848cec7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T22:40:10.782747Z",
     "start_time": "2026-01-06T22:40:09.528508Z"
    }
   },
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "bb05dc5d-27dc-4339-9155-f5d9abc9eba7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T22:40:11.569304Z",
     "start_time": "2026-01-06T22:40:11.137430Z"
    }
   },
   "source": [
    "# Training loop\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(index_flat, offsets)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200], Loss: 1.0126\n",
      "Epoch [40/200], Loss: 0.8235\n",
      "Epoch [60/200], Loss: 0.5743\n",
      "Epoch [80/200], Loss: 0.3375\n",
      "Epoch [100/200], Loss: 0.1797\n",
      "Epoch [120/200], Loss: 0.0985\n",
      "Epoch [140/200], Loss: 0.0599\n",
      "Epoch [160/200], Loss: 0.0405\n",
      "Epoch [180/200], Loss: 0.0297\n",
      "Epoch [200/200], Loss: 0.0229\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "f443ddcf-3146-4f30-9c7c-068f2826051c",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2026-01-06T22:40:11.609125Z",
     "start_time": "2026-01-06T22:40:11.599632Z"
    }
   },
   "source": [
    "# Testing the classifier\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_sentence = \"I hate cats and dogs\"\n",
    "    test_tokens = torch.tensor(vocab(tokenizer(test_sentence)))\n",
    "    test_offset = torch.tensor([0])\n",
    "    \n",
    "    output = model(test_tokens, test_offset)\n",
    "    predicted_label = torch.argmax(output).item()\n",
    "    \n",
    "    sentiment_map = {0: \"Negative\", 1: \"Positive\", 2: \"Neutral\"}\n",
    "    print(f\"Sentence: '{test_sentence}' -> Sentiment: {sentiment_map[predicted_label]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'I hate cats and dogs' -> Sentiment: Negative\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T22:40:11.647516Z",
     "start_time": "2026-01-06T22:40:11.630611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the trained model and vocabulary - VERSION COMPATIBLE\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'vocab_stoi': vocab.get_stoi(),  # Save as plain dict\n",
    "    'vocab_itos': vocab.get_itos(),  # Save as plain list\n",
    "    'embedding_dim': embedding_dim,\n",
    "    'num_classes': num_classes\n",
    "}, 'sentiment_model.pth')\n",
    "\n",
    "print(\"✓ Model saved successfully as 'sentiment_model.pth'\")"
   ],
   "id": "10f27c6154be62cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model saved successfully as 'sentiment_model.pth'\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4971068-35b8-4bfc-87e6-89fef2a02232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def classify_review(review):\n",
    "    \"\"\"Classify sentiment of a review using the trained model.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_tokens = torch.tensor(vocab(tokenizer(review)))\n",
    "        test_offset = torch.tensor([0])\n",
    "        output = model(test_tokens, test_offset)\n",
    "        predicted_label = torch.argmax(output).item()\n",
    "    return predicted_label  # Returns 0 (Negative), 1 (Positive), or 2 (Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30250f00-dfa8-4289-a2dd-bb46df46cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_negative_review(name, review, filename=\"negative_reviews.csv\"):\n",
    "    \"\"\"Save negative reviews to a CSV file.\"\"\"\n",
    "    file_exists = os.path.exists(filename)\n",
    "\n",
    "    # Create DataFrame from new entry\n",
    "    new_entry = pd.DataFrame([[name, review]], columns=[\"Name\", \"Review\"])\n",
    "\n",
    "    # Append to CSV, creating it if necessary\n",
    "    if file_exists:\n",
    "        new_entry.to_csv(filename, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        new_entry.to_csv(filename, mode=\"w\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "675ff34f-a479-4da9-88c0-16fb28ae71db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your name:  Anointina\n",
      "Share your review of the product:  I hate dogs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your review was classified as: Negative\n",
      "Your negative review has been recorded.\n"
     ]
    }
   ],
   "source": [
    "# --- User Interaction ---\n",
    "name = input(\"Enter your name: \")\n",
    "review = input(\"Share your review of the product: \")\n",
    "\n",
    "sentiment_label = classify_review(review)\n",
    "sentiment_map = {0: \"Negative\", 1: \"Positive\", 2: \"Neutral\"}\n",
    "print(f\"\\nYour review was classified as: {sentiment_map[sentiment_label]}\")\n",
    "\n",
    "# Save negative reviews\n",
    "if sentiment_label == 0:\n",
    "    save_negative_review(name, review)\n",
    "    print(\"Your negative review has been recorded.\")\n",
    "else:\n",
    "    print(\"Thank you for your feedback!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
