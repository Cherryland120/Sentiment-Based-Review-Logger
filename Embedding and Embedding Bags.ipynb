{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc329daf-f404-4c6c-8ce9-6d8e19315bed",
   "metadata": {},
   "source": [
    "# Natural Language Processing with PyTorch and TorchText\n",
    "\n",
    "This script demonstrates the basic workflow of tokenizing text data, building a vocabulary, and generating word embeddings using PyTorch's nn.Embedding and nn.EmbeddingBag modules.\n",
    "\n",
    "## Key Features:\n",
    "- Tokenization using SpaCy tokenizer via TorchText\n",
    "- Vocabulary construction from a dataset of sample sentences\n",
    "- Generation of word embeddings for individual words and sentences\n",
    "- Introduction to handling variable-length sequences with offsets in EmbeddingBag\n",
    "\n",
    "## Dependencies:\n",
    "- torch\n",
    "- torchtext\n",
    "- spacy ('en_core_web_sm' language model)\n",
    "\n",
    "Author: Tamunowunari-Tasker Anointing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bdeb817-5570-48cd-b4c3-1d4553f9842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries from PyTorch and TorchText\n",
    "import torchtext.data.utils as tdu  # For tokenization utilities\n",
    "import torchtext.vocab as tv        # For vocabulary building\n",
    "import torch.nn as nn               # For neural network components like Embedding\n",
    "import torch                        # For tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebd92f85-8242-4c80-b625-f9caecfc7899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset of simple sentences\n",
    "dataset = [\n",
    "    \"I like cats\",\n",
    "    \"I hate dogs\",\n",
    "    \"I'm impartial to hippos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c52c6943-4444-4fa4-9d49-ddce6effaa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer using SpaCy's small English model\n",
    "tokenizer = tdu.get_tokenizer('spacy', language = 'en_core_web_sm') # get_tokenizer() is unique to torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "799feb94-cab3-4a55-8b99-c6a8fcf9d629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to yield tokens from each sentence in the dataset\n",
    "def yield_tokens(data_iter):\n",
    "    \"\"\"\n",
    "    Tokenizes each sentence in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        data_iter (iter): An iterator over the dataset.\n",
    "        \n",
    "    Yields:\n",
    "        list: A list of tokens for each sentence.\n",
    "    \"\"\"\n",
    "    for data_sample in data_iter:\n",
    "        yield tokenizer(data_sample) # gives you one value at a time but remembers where it left off, ready to continue when called again.\n",
    "\n",
    "# Create an iterator over the dataset\n",
    "data_iter = iter(dataset) # iter is an inbuilt function in Python that returns an iterator from a list, tuple or a custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96a7ce61-c232-4595-a92f-0667a21eeb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a vocabulary from the tokenized dataset\n",
    "vocab = tv.build_vocab_from_iterator(yield_tokens(data_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37a7a600-2b9a-432f-9dca-b64cb9fe29bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'cats', 'dogs', 'hate', 'hippos', 'impartial', 'like', 'to']\n"
     ]
    }
   ],
   "source": [
    "# Display the vocabulary as a list of words (index-to-string mapping)\n",
    "print(vocab.get_itos()) # get_itos() returns the list/bag of words. This results in a bag of 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "893442fd-228a-4e79-a227-f167395d308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A lambda in Python is just a one-liner function. Itâ€™s a shortcut for simple operations.                                                               \n",
    "input_indexes = lambda x: [torch.tensor(vocab(tokenizer(data_sample))) for data_sample in x]\n",
    "\n",
    "# Just for comparison\n",
    "def input_indexes(x):\n",
    "    result = []\n",
    "    for data_sample in x:\n",
    "        tokens = tokenizer(data_sample)\n",
    "        indices = vocab(tokens)\n",
    "        tensor = torch.tensor(indices)\n",
    "        result.append(tensor)\n",
    "    return result\n",
    "# Same effect, but more lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d68b787a-0ba8-4d06-b0de-f3fed703ecd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token indices: [tensor([0, 7, 2]), tensor([0, 4, 3]), tensor([0, 1, 6, 8, 5])]\n"
     ]
    }
   ],
   "source": [
    "# Convert dataset sentences to token indices\n",
    "index = input_indexes(dataset) # index has three tensors\n",
    "print(\"Token indices:\", index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b51cb7d-e31b-406e-b04c-6888e2a37176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set embedding dimensions (number of features each word vector will have)\n",
    "embedding_dim = 3 # Each word will be represented in a 3D vector space\n",
    "\n",
    "# Number of unique words in the vocabulary\n",
    "n_embedding = len(vocab)\n",
    "#        OR\n",
    "n_embedding = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d46cecac-9279-44d0-be88-c47f7a5b01c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(9, 3)\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedding layer\n",
    "embeds = nn.Embedding(n_embedding, embedding_dim)\n",
    "print(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85d2358f-6c19-4146-88e1-695cb6c4f53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings for I like cats:  tensor([[-0.4450, -1.2641, -0.1237],\n",
      "        [ 0.1213,  2.2594, -0.3364],\n",
      "        [-0.7020,  0.1398, -1.4275]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example: Get embeddings for the sentence \"I like cats\"\n",
    "i_like_cats = embeds(index[0])\n",
    "print(\"Embeddings for I like cats: \", i_like_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33e2da2a-efa7-44c5-bffb-08d89af09b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for I'm impartial to hippos:  tensor([[-0.4450, -1.2641, -0.1237],\n",
      "        [-0.0810,  2.2538,  0.5334],\n",
      "        [ 2.2675,  0.5273,  0.0998],\n",
      "        [ 0.0560, -0.6253, -0.4342],\n",
      "        [ 1.8003,  0.3011, -0.3945]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example: Get embeddings for \"I'm impartial to hippos\"\n",
    "impartial_to_hippos = embeds(index[-1]) # -1 is the equivalent of [2] or the last set which is the hippos one\n",
    "print(\"Embedding for I'm impartial to hippos: \", impartial_to_hippos)\n",
    "# \"I'm impartial to hippos\" is [5] words (I is part) by the [3] neurons of the next layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "804dcd73-77b1-4048-9451-7beba3668c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 7, 2, 0, 4, 3, 0, 1, 6, 8, 5])\n"
     ]
    }
   ],
   "source": [
    "# Flatten all sentence indices into a single tensor\n",
    "index_flat = torch.cat(index)\n",
    "print(index_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28f25b8a-6b75-4891-b05e-0b49c848cec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "# Calculate offsets to indicate the start of each sentence in the flattened tensor\n",
    "offset = [len(sample) for sample in index]\n",
    "offset.insert(0,0) # Insert 0 at the start to indicate the first sentence\n",
    "print(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb05dc5d-27dc-4339-9155-f5d9abc9eba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "# Using cummulative sum we show where the next sentence begins (position 0, position 3, position 6)\n",
    "offset = torch.tensor(offset)\n",
    "\n",
    "# Compute cumulative sum to get starting positions for each sentence\n",
    "offset = torch.cumsum(offset, 0)[0:-1] # Slices of the last element as not needed\n",
    "print(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f443ddcf-3146-4f30-9c7c-068f2826051c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2888, -0.1475, -0.1623],\n",
      "        [-0.8679, -0.0473,  0.4397],\n",
      "        [-0.1703, -0.2355, -0.2356]], grad_fn=<EmbeddingBagBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Initialize EmbeddingBag layer, which computes sentence embeddings by averaging word vectors\n",
    "embedding_bag = nn.EmbeddingBag(num_embeddings=n_embedding, embedding_dim=embedding_dim, mode='mean')\n",
    "\n",
    "# Compute embeddings for entire sentences\n",
    "sentence_embeddings = embedding_bag(index_flat, offsets = offset)\n",
    "print(sentence_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
